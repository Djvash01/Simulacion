{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final\n",
    "\n",
    "###  CLASIFICACION CON GMMs\n",
    "\n",
    "### Universidad de Antioquia\n",
    "\n",
    "### Facultad de Ingeniería\n",
    "\n",
    "### Ingeniería de Sistemas\n",
    "\n",
    "### Ude@ - 2018-I\n",
    "\n",
    "#### Profesor: Antonio Tamayo Herrera\n",
    "\n",
    "### Integrantes:\n",
    "\n",
    "Nombre: David de Jesus Yepes Isaza\n",
    "\n",
    "Cédula: 1020455542\n",
    "\n",
    "Nombre: Rodrigo Antonio Porras Martinez\n",
    "\n",
    "Cédula: 1039459720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación en el dataset \"Heart Disease Data Set\" con GaussianMixture\n",
    "\n",
    "La base de datos con las que se trabaja en el presente artículo fue extraída de  Cleveland Clinic Foundation y fue creada por V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "El set de datos de de Cleveland consta de 303 instancias, la variable de salida o etiqueta son 0 para la ausencia y los valores 1, 2, 3 y 4 para la presencia de enfermedades. \n",
    " \n",
    "El 54% de las instancias tienen ausencia de enfermedades y 46% tienen presencia de enfermedades del corazón. Para simplificar el problema se analiza como un problema biclase donde  “0” son personas sanas y “1” que corresponde a (1,2,3,4) son personas enfermas.\n",
    "\n",
    "El objetivo es según las características de cada paciente identificar cuál de ellos presenta presencia de enfermedades y cuáles no.\n",
    "\n",
    "La base de datos contiene 303 muestras con las 14 características siguientes:\n",
    "1. N° 3 (edad)   \n",
    "2. N° 4 (sexo) \n",
    "3. N° 9 (cp) tipo de dolor en el pecho\n",
    "        Valor 1: angina típica\n",
    "        Valor 2: angina atípica\n",
    "        Valor 3: dolor no anginal\n",
    "        Valor 4: asintomático  \n",
    "4. N° 10 (trestbps) presión arterial en reposo\n",
    "5. N° 12 (chol) colesterol sérico en mg / dl (miligramo por decilitro)\n",
    "6. N° 16 (fbs) (azúcar en la sangre en ayunas > 120 mg / dl) (1 = verdadero, 0 = falso)\n",
    "7. N° 19 (restecg)  resultados electrocardiográficos en reposo:\n",
    "        Valor 0: normal\n",
    "        Valor 1: tiene anormalidad de onda ST-T (inversiones de onda T y / o ST  elevación o depresión de > 0.05 mV)\n",
    "        Valor 2: muestra hipertrofia ventricular izquierda probable o definitiva según los criterios de Estes  \n",
    "8. N° 32 (thalach)  frecuencia cardíaca máxima lograda\n",
    "9. N° 38 (exang) angina inducida por el ejercicio (1 = sí; 0 = no)\n",
    "10. N° 40 (oldpeak) depresión del ST inducida por el ejercicio en relación con el reposo\n",
    "11. N° 41 (pendiente) la pendiente del segmento ST de ejercicio máximo\n",
    "        Valor 1: pendiente ascendente\n",
    "        Valor 2: plano\n",
    "        Valor 3: downsloping\n",
    "12. N° 44 (ca) número de vasos principales (0-3) coloreados por fluoroscopia\n",
    "13. N° 51 (thal) 3 = normal; 6 = defecto fijo; 7 = defecto reversible\n",
    "14. N° 58 (num) (el atributo predicho) diagnóstico de enfermedad cardíaca (estado angiográfico de la enfermedad)\n",
    "        - Valor 0: <50% de diámetro de estrechamiento\n",
    "        - Valor 1:> 50% de diámetro de estrechamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion de Librerias y error de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from __future__ import division\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def classification_error(y_est, y_real):\n",
    "    err = 0\n",
    "    for y_e, y_r in zip(y_est, y_real):\n",
    "\n",
    "        if y_e != y_r:\n",
    "            err += 1\n",
    "\n",
    "    return err/np.size(y_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminacion de valores Perdidos (Missing Values)\n",
    "\n",
    "En la base de datos algunos valores están perdidos, se pueden identificar por el valor “-9.0”. por lo que se deben limpiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim de la base de datos original: (303, 14)\n",
      "\n",
      "Hay 0 valores perdidos en la variable de salida. :(\n",
      "\n",
      "Dim de la base de datos sin las muestras con variable de salida perdido (303, 14)\n",
      "\n",
      "Procesando imputación de valores perdidos en las características . . .\n",
      "\n",
      "Imputación finalizada.\n",
      "\n",
      "No hay valores perdidos en la base de datos. Ahora se puede procesar. La base de datos está en la variable DataBase\n",
      "Dimension: (303, 14)\n"
     ]
    }
   ],
   "source": [
    "#cargamos la bd de entrenamiento\n",
    "db = np.loadtxt('cleveland.txt',delimiter=',')  # El delimitador es \",\"\n",
    "print( \"Dim de la base de datos original: \" + str(np.shape(db)))\n",
    "db = db.reshape(303,14)\n",
    "DataBase = db\n",
    "#La base d edatos processed.cleveland tiene valores perdidos, vamos a quitar todos esos registros de la base de datos\n",
    "#Están marcados como -9.0. Donde haya un valor -9.0 eliminaremos el registro.\n",
    "j = 0\n",
    "for i in range(0,np.size(db,0)):\n",
    "    if -9.0 == db[i,13]:\n",
    "        j+=1\n",
    "        DataBase = np.delete(DataBase,i,0)\n",
    "    elif 1 <= db[i,13]: #Se cambia la variables de salida (1,2,3 y 4) por el valor 1 para obtener un problema biclase\n",
    "        DataBase[i,13]=1\n",
    "    \n",
    "print (\"\\nHay \" + str(j) + \" valores perdidos en la variable de salida. :(\")\n",
    "\n",
    "print (\"\\nDim de la base de datos sin las muestras con variable de salida perdido \"+ str(np.shape(DataBase)))\n",
    "\n",
    "#Ya hemos eliminado los registros con valor de la variable de salida perdido.\n",
    "\n",
    "#Ahora vamos a imputar los valores perdidos en cada una de las características\n",
    "print (\"\\nProcesando imputación de valores perdidos en las características . . .\\n\")\n",
    "\n",
    "for k in range(0,np.size(DataBase,0)):\n",
    "    for w in range(0,14):\n",
    "        if -9.0 == DataBase[k,w]:\n",
    "            DataBase[k,w] = round(np.mean(DataBase[:,w]))\n",
    "        \n",
    "print (\"Imputación finalizada.\\n\")\n",
    "\n",
    "hay_missed_values = False\n",
    "for i in range(0,np.size(DataBase,0)):\n",
    "    if -9.0 in DataBase[i,:]:\n",
    "        hay_missed_values = True\n",
    "if(hay_missed_values):\n",
    "    print (\"Hay valores perdidos\")\n",
    "else:\n",
    "    print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar. La base de datos está en la variable DataBase\")\n",
    "\n",
    "print(\"Dimension: \"+ str(np.shape(DataBase)))\n",
    "#print(DataBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error de validación: 0.42591397849462365 +/- 0.128498402787815\n",
      "\n",
      "Sensibilidad: 0.228485803412274\n",
      "\n",
      "Especificidad: 0.8657686495650893\n",
      "\n",
      "Precision: 0.5875\n",
      "\n",
      "Eficiencia: 0.5740860215053762\n",
      "\n",
      "\n",
      "Tiempo total de ejecución: 0.2389240264892578 segundos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cargamos la bd de entrenamiento  despues de tenerla limpia\n",
    "\n",
    "X = DataBase[:,0:13]\n",
    "\n",
    "Y = db[:,13]\n",
    "\n",
    "#print (np.shape(Y))\n",
    "\n",
    "#Normalizamos las caracterisiticas\n",
    "X = normalize(X, axis=0, norm='l1')\n",
    "\n",
    "#Para calcular el costo computacional\n",
    "tiempo_i = time.time()\n",
    "\n",
    "#Para obtener el numero de clases\n",
    "n_classes = len(np.unique(Y))\n",
    "#print(n_classes)\n",
    "\n",
    "estimator = GaussianMixture(n_components=n_classes, covariance_type='full')\n",
    "\n",
    "#Implemetamos la metodología de validación cross validation con 10 folds\n",
    "\n",
    "Errores = np.ones(10)\n",
    "Sensibilidad = np.ones(10)\n",
    "Especificidad = np.ones(10)\n",
    "Precision = np.ones(10)\n",
    "Eficiencia = np.ones(10)\n",
    "\n",
    "j = 0\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]  \n",
    "    \n",
    "    #Con la base de datos se realiza una inicialización de las medias de cada Gaussiana de manera supervisada\n",
    "    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "   \n",
    "    # Entrenar el modelo.\n",
    "    estimator.fit(X_train)\n",
    "    \n",
    "    # Validación del modelo\n",
    "    ypred = estimator.predict(X_test)\n",
    "\n",
    "    Errores[j] = classification_error(ypred, y_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, ypred).ravel()\n",
    "    Sensibilidad[j] = tp/(tp+fn)\n",
    "    Especificidad[j] = tn/(tn+fp)\n",
    "    Precision[j] = tp/(tp+fp)\n",
    "    Eficiencia[j] = (tp+tn)/(tp+tn+fp+fn)\n",
    "    j+=1\n",
    "    \n",
    "print(\"\\nError de validación: \" + str(np.mean(Errores)) + \" +/- \" + str(np.std(Errores)))\n",
    "print(\"\\nSensibilidad: \"+str(np.mean(Sensibilidad)))\n",
    "print(\"\\nEspecificidad: \"+str(np.mean(Especificidad)))\n",
    "print(\"\\nPrecision: \"+str(np.mean(Precision)))\n",
    "print(\"\\nEficiencia: \"+str(np.mean(Eficiencia)))\n",
    "print ((\"\\n\\nTiempo total de ejecución: \" + str(time.time()-tiempo_i)) + \" segundos.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Clases</th>\n",
    "    <th>var-cov</th>\n",
    "    <th>Sensibilidad</th>\n",
    "    <th>Especificidad</th>\n",
    "    <th>Precision</th>\n",
    "    <th>Eficiencia</th>\n",
    "    <th>Error de validación</th>\n",
    "    <th>Tiempo de ejecución</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Full</td>\n",
    "    <td>0.33974917239623126</td>\n",
    "    <td>0.7187761683620817</td>\n",
    "    <td>0.5308333333333333</td>\n",
    "    <td>0.5474193548387096</td>\n",
    "    <td>0.3959139784946237 +/- 0.1488847817229718</td>\n",
    "    <td>0.5328292846679688 segundos.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Diag</td>\n",
    "      <td>0.6047373949579833</td>\n",
    "      <td>0.8497160179861417</td>\n",
    "      <td>0.7811546786546787</td>\n",
    "      <td>0.7330107526881721</td>\n",
    "    <td>0.26698924731182794 +/- 0.06459323349538117</td>\n",
    "    <td>0.06897854804992676 segundos.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Spherical</td>\n",
    "      <td>0.45680831423478485</td>\n",
    "      <td>0.6493579395320881</td>\n",
    "      <td>0.5291179408826467</td>\n",
    "      <td>0.554516129032258</td>\n",
    "    <td>0.44548387096774195 +/- 0.11675463706812943</td>\n",
    "    <td>0.07297635078430176 segundos.</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el anterior podemos concluir que el modelo GMMs no obtiene muy buenos resultados con nuestra base de datos, el mejor resultado lo obtuvo con la matriz de covarianzas diagonal con una eficiencia de 0.73. El costo computacional no es alto, pero debemos recordar que son solo 303 muestras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
