{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final\n",
    "\n",
    "###  CLASIFICACION CON RANDOM FOREST\n",
    "\n",
    "### Universidad de Antioquia\n",
    "\n",
    "### Facultad de Ingeniería\n",
    "\n",
    "### Ingeniería de Sistemas\n",
    "\n",
    "### Ude@ - 2018-I\n",
    "\n",
    "#### Profesor: Antonio Tamayo Herrera\n",
    "\n",
    "### Integrantes:\n",
    "\n",
    "Nombre: David de Jesus Yepes Isaza\n",
    "\n",
    "Cédula: 1020455542\n",
    "\n",
    "Nombre: Rodrigo Antonio Porras Martinez\n",
    "\n",
    "Cédula: 1039459720\n",
    "\n",
    "### Clasificación en el dataset \"Heart Disease Data Set\" con RANDOM FOREST\n",
    "\n",
    "La base de datos con las que se trabaja en el presente artículo fue extraída de  Cleveland Clinic Foundation y fue creada por V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "El set de datos de de Cleveland consta de 303 instancias, la variable de salida o etiqueta son 0 para la ausencia y los valores 1, 2, 3 y 4 para la presencia de enfermedades. \n",
    " \n",
    "El 54% de las instancias tienen ausencia de enfermedades y 46% tienen presencia de enfermedades del corazón. Para simplificar el problema se analiza como un problema biclase donde  “0” son personas sanas y “1” que corresponde a (1,2,3,4) son personas enfermas.\n",
    "\n",
    "El objetivo es según las características de cada paciente identificar cuál de ellos presenta presencia de enfermedades y cuáles no.\n",
    "\n",
    "La base de datos contiene 303 muestras con las 14 características siguientes:\n",
    "1. N° 3 (edad)   \n",
    "2. N° 4 (sexo) \n",
    "3. N° 9 (cp) tipo de dolor en el pecho\n",
    "        Valor 1: angina típica\n",
    "        Valor 2: angina atípica\n",
    "        Valor 3: dolor no anginal\n",
    "        Valor 4: asintomático  \n",
    "4. N° 10 (trestbps) presión arterial en reposo\n",
    "5. N° 12 (chol) colesterol sérico en mg / dl (miligramo por decilitro)\n",
    "6. N° 16 (fbs) (azúcar en la sangre en ayunas > 120 mg / dl) (1 = verdadero, 0 = falso)\n",
    "7. N° 19 (restecg)  resultados electrocardiográficos en reposo:\n",
    "        Valor 0: normal\n",
    "        Valor 1: tiene anormalidad de onda ST-T (inversiones de onda T y / o ST  elevación o depresión de > 0.05 mV)\n",
    "        Valor 2: muestra hipertrofia ventricular izquierda probable o definitiva según los criterios de Estes  \n",
    "8. N° 32 (thalach)  frecuencia cardíaca máxima lograda\n",
    "9. N° 38 (exang) angina inducida por el ejercicio (1 = sí; 0 = no)\n",
    "10. N° 40 (oldpeak) depresión del ST inducida por el ejercicio en relación con el reposo\n",
    "11. N° 41 (pendiente) la pendiente del segmento ST de ejercicio máximo\n",
    "        Valor 1: pendiente ascendente\n",
    "        Valor 2: plano\n",
    "        Valor 3: downsloping\n",
    "12. N° 44 (ca) número de vasos principales (0-3) coloreados por fluoroscopia\n",
    "13. N° 51 (thal) 3 = normal; 6 = defecto fijo; 7 = defecto reversible\n",
    "14. N° 58 (num) (el atributo predicho) diagnóstico de enfermedad cardíaca (estado angiográfico de la enfermedad)\n",
    "        - Valor 0: <50% de diámetro de estrechamiento\n",
    "        - Valor 1:> 50% de diámetro de estrechamiento\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importacion de librerias y limpiado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim de la base de datos original: (303, 14)\n",
      "\n",
      "Hay 0 valores perdidos en la variable de salida.\n",
      "\n",
      "Dim de la base de datos sin las muestras con variable de salida perdido (303, 14)\n",
      "Se han reseteado a 1 139 valores ya que eran mayores a 0\n",
      "\n",
      "Procesando imputación de valores perdidos en las características . . .\n",
      "\n",
      "Imputación finalizada.\n",
      "\n",
      "No hay valores perdidos en la base de datos. Ahora se puede procesar. La base de datos está en la variable DataBase\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from __future__ import division\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "\n",
    "def class_error(y_est, y_real):\n",
    "    err = 0\n",
    "    for y_e, y_r in zip(y_est, y_real):\n",
    "        if y_e != y_r:\n",
    "            err += 1\n",
    "\n",
    "    return \"accuracy: \" + str(1-(err/np.size(y_est)))\n",
    "\n",
    "def classification_error(y_est, y_real):\n",
    "    err = 0\n",
    "    for y_e, y_r in zip(y_est, y_real):\n",
    "        if y_e != y_r:\n",
    "            err += 1\n",
    "    return err/np.size(y_est)\n",
    "\n",
    "clevelanddb = np.loadtxt('cleveland.txt',delimiter=',')  # Assuming comma-delimiter\n",
    "print (\"Dim de la base de datos original: \" + str(np.shape(clevelanddb)))\n",
    "DataBase = clevelanddb\n",
    "#La base de datos cleveland.txt tiene algunos valores perdidos, estos se identifican con -9.0\n",
    "#Los valores de salida mayores a 0 se considerarán 1 de esta forma la clasificacion será biclase \n",
    "j = 0\n",
    "num_reset_values = 0\n",
    "\n",
    "for i in range(0,np.size(clevelanddb,0)):\n",
    "    if -9.0 == clevelanddb[i,13]:\n",
    "        j+=1\n",
    "        DataBase = np.delete(DataBase,i,0)\n",
    "    elif clevelanddb[i,13] > 0:\n",
    "        clevelanddb[i,13] = 1\n",
    "        num_reset_values += 1\n",
    "\n",
    "print (\"\\nHay \" + str(j) + \" valores perdidos en la variable de salida.\")\n",
    "\n",
    "print (\"\\nDim de la base de datos sin las muestras con variable de salida perdido \" + str(np.shape(DataBase)))\n",
    "\n",
    "print (\"Se han reseteado a 1 \"+str(num_reset_values)+\" valores ya que eran mayores a 0\")\n",
    "\n",
    "#Ya hemos eliminado los registros con valor de la variable de salida perdido.\n",
    "\n",
    "#Ahora vamos a imputar los valores perdidos en cada una de las características\n",
    "print (\"\\nProcesando imputación de valores perdidos en las características . . .\\n\")\n",
    "\n",
    "for k in range(0,np.size(DataBase,0)):\n",
    "    for w in range(0,14):\n",
    "        if -9.0 == DataBase[k,w]:\n",
    "            DataBase[k,w] = round(np.mean(DataBase[:,w]))\n",
    "        \n",
    "print (\"Imputación finalizada.\\n\")\n",
    "\n",
    "hay_missed_values = False\n",
    "for i in range(0,np.size(DataBase,0)):\n",
    "    if -9.0 in DataBase[i,:]:\n",
    "        hay_missed_values = True\n",
    "if(hay_missed_values):\n",
    "    print (\"Hay valores perdidos\")\n",
    "else:\n",
    "    print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar. La base de datos está en la variable DataBase\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando Random Forest con 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecucion con criterion: entropy y splliter: random\n",
      "\n",
      "Error de validación: 0.27419354838709675 +/- 0.08087851543083463\n",
      "\n",
      "Sensibilidad: 0.7072816399286987\n",
      "\n",
      "Especificidad: 0.7328643157667926\n",
      "\n",
      "Precision: 0.6984031164913518\n",
      "\n",
      "Eficiencia : 0.7258064516129031\n",
      " \n",
      "\n",
      "\n",
      "Tiempo total de ejecución: 0.0236690044403 segundos.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = DataBase[:,0:13]\n",
    "Y = DataBase[:,13]\n",
    "\n",
    "#Normalizamos los datos\n",
    "X = normalize(X, axis=0, norm='l1')\n",
    "\n",
    "#Implemetamos la metodología de validación cross validation con 10 folds\n",
    "\n",
    "#Para calcular el costo computacional\n",
    "tiempo_i = time.time()\n",
    "\n",
    "Sensibilidad = np.ones(10)\n",
    "Especificidad = np.ones(10)\n",
    "Precision = np.ones(10)\n",
    "Eficiencia = np.ones(10)\n",
    "Errores = np.ones(10) \n",
    "\n",
    "j = 0\n",
    "fold_num = 0\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "m_criterion = \"entropy\"\n",
    "m_splitter = \"random\"\n",
    "clf = tree.DecisionTreeClassifier(criterion = m_criterion,splitter = m_splitter)\n",
    "\n",
    "print(\"Ejecucion con criterion: \"+m_criterion+\" y splliter: \"+m_splitter)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]  \n",
    "    fold_num += 1\n",
    "    #Complete el código aquí creando el modelo de árbol clasificador y entrenandolo unicamente con las muestras de entrenamiento\n",
    "    #de la base de datos\n",
    "\n",
    "    \n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    #Complete el código aquí prediciendo la clase para las muestras de validación (test) de la base de datos.\n",
    "    y_pred = clf.predict(X_test)\n",
    "      \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,y_pred ).ravel()\n",
    "    \n",
    "    Errores[j] = classification_error(y_pred, y_test)\n",
    "    Sensibilidad[j] = tp/(tp+fn)\n",
    "    Especificidad[j] = tn/(tn+fp)\n",
    "    Precision[j] = tp/(tp+fp)\n",
    "    Eficiencia[j] = (tp+tn)/(tp+tn+fp+fn)\n",
    "    j+=1\n",
    "    \n",
    "print(\"\\nError de validación: \" + str(np.mean(Errores)) + \" +/- \" + str(np.std(Errores)))\n",
    "print(\"\\nSensibilidad: \"+str(np.mean(Sensibilidad)))\n",
    "print(\"\\nEspecificidad: \"+str(np.mean(Especificidad)))\n",
    "print(\"\\nPrecision: \"+str(np.mean(Precision)))\n",
    "print(\"\\nEficiencia : \"+str(np.mean(Eficiencia)) + \"\\n \")\n",
    "print (\"\\n\\nTiempo total de ejecución: \" + str(time.time()-tiempo_i) + \" segundos.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>CRITERION</th>\n",
    "    <th>SPLITTER</th>\n",
    "    <th>ERROR DE VALIDACION</th>\n",
    "    <th>SENSIBILIDAD</th>\n",
    "    <th>ESPECIFICIDAD</th>\n",
    "    <th>PRECISION</th>\n",
    "    <th>EFICIENCIA</th>\n",
    "    <th>TIEMPO TOTAL DE EJCUCION</th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gini</td>\n",
    "    <td>best</td>\n",
    "    <td>0.2811827956989247 +/- 0.10443429027424364</td>\n",
    "    <td>0.7021559078176726</td>\n",
    "    <td>0.7330883912269361</td>\n",
    "    <td>0.6913932558669401</td>\n",
    "    <td>0.7188172043010752</td>\n",
    "    <td>0.0327839851379 segundos.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>gini</td>\n",
    "    <td>random</td>\n",
    "    <td>0.2738709677419354 +/- 0.06464154476609289</td>\n",
    "    <td>0.7063015660809778</td>\n",
    "    <td>0.7490978294151669</td>\n",
    "    <td>0.702136393018746</td>\n",
    "    <td>0.7261290322580646</td>\n",
    "    <td>0.0201580524445 segundos.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>entropy</td>\n",
    "    <td>best</td>\n",
    "    <td>0.303763440860215 +/- 0.09286644283777461</td>\n",
    "    <td>0.6664915966386554</td>\n",
    "    <td>0.7191905102121819</td>\n",
    "    <td>0.6599999999999999</td>\n",
    "    <td>0.696236559139785</td>\n",
    "    <td>0.0279319286346 segundos.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>entropy</td>\n",
    "    <td>random</td>\n",
    "    <td>0.22139784946236557 +/- 0.047854778006057225</td>\n",
    "    <td>0.7232505729564553</td>\n",
    "    <td>0.8241972578505086</td>\n",
    "    <td>0.785277639027639</td>\n",
    "    <td>0.7786021505376344</td>\n",
    "    <td>0.0225191116333 segundos.</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Caracteristicas - Wrappers\n",
    "\n",
    "Para selecionar las caracteristicas usaremos la funcion Wrappers Random Forest. los parámetros son el número de características a seleccionar  (3, 7 y 10) y la estrategia a implementar (SFS, SBS, SFFS, SBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s finished\n",
      "Features: 1/7[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "Features: 2/7[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "Features: 3/7[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "Features: 4/7[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "Features: 5/7[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "Features: 6/7[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "Features: 7/7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error de validación aplicando SFS: 0.0 +/- 0.0\n",
      "\n",
      "Eficiencia en validación aplicando SFS: 100.0%\n",
      "\n",
      "\n",
      "Tiempo total de ejecución: 1.8153450489 segundos.\n",
      "Las caracteristicas seleccionadas son: \n",
      "1. edad\n",
      "2. sexo\n",
      "3. (cp) tipo de dolor en el pecho\n",
      "4. (trestbps) presión arterial en reposo\n",
      "5. (chol) colesterol sérico en mg / dl (miligramo por decilitro)\n",
      "6. (fbs) (azúcar en la sangre en ayunas > 120 mg / dl) (1 = verdadero, 0 = falso)\n",
      "8. (thalach) frecuencia cardíaca máxima lograda\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Feature Selection Function\n",
    "#Recibe 4 parámetros: 1. el modelo (clf para nuestro caso), 2. el número de características final que se quiere alcanzar\n",
    "#3. Si es forward (True), si es Backward False, 4. Si es es flotante (True), sino False\n",
    "def select_features(modelo, n_features, fwd, fltg):\n",
    "    \n",
    "    sfs = SFS(modelo, \n",
    "           k_features=n_features, \n",
    "           forward=fwd,\n",
    "           floating=fltg,\n",
    "           verbose=1,\n",
    "           scoring='accuracy',\n",
    "           cv=0)\n",
    "    \n",
    "    return sfs\n",
    "\n",
    "\n",
    "#Para calcular el costo computacional\n",
    "tiempo_i = time.time()\n",
    "\n",
    "#Implemetamos la metodología de validación cross validation con 10 folds\n",
    "\n",
    "j = 0\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "Errores = np.ones(10) \n",
    "\n",
    "\n",
    "m_criterion = \"entropy\"\n",
    "m_splitter = \"random\"\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]  \n",
    "   \n",
    "    #Aquí se entrena y se valida el modelo haciendo selección de características con diferentes estrategias\n",
    "    \n",
    "    #Complete el código llamando el método select_features con los parámetros correspondientes para responder el\n",
    "    #Ejercicio 3.1\n",
    "    #SFS True False, SBS False False, SFFS True True, SBFS False True\n",
    "    sf = select_features(clf,7,True,False)\n",
    "\n",
    "    #Complete el código para entrenar el modelo con las características seleccionadas. Tenga en cuenta\n",
    "    #la metodología de validación aplicada para que pase las muestras de entrenamiento correctamente.\n",
    "    sf = sf.fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    Errores[j] = 1-sf.k_score_\n",
    "    j+=1\n",
    "\n",
    "\n",
    "print(\"\\nError de validación aplicando SFS: \" + str(np.mean(Errores)) + \" +/- \" + str(np.std(Errores)))\n",
    "\n",
    "print(\"\\nEficiencia en validación aplicando SFS: \" + str(sf.k_score_*100) + \"%\" )\n",
    "\n",
    "print (\"\\n\\nTiempo total de ejecución: \" + str(time.time()-tiempo_i) + \" segundos.\")\n",
    "\n",
    "print(\"Las caracteristicas seleccionadas son: \")\n",
    "\n",
    "dictChar = {0 : \"1. edad\", \n",
    "            1 : '2. sexo',\n",
    "            2 : '3. (cp) tipo de dolor en el pecho',\n",
    "            3 : '4. (trestbps) presión arterial en reposo',\n",
    "            4 : '5. (chol) colesterol sérico en mg / dl (miligramo por decilitro)',\n",
    "            5 : '6. (fbs) (azúcar en la sangre en ayunas > 120 mg / dl) (1 = verdadero, 0 = falso)',\n",
    "            6 : '7. (restecg) resultados electrocardiográficos en reposo:',\n",
    "            7 : '8. (thalach) frecuencia cardíaca máxima lograda',\n",
    "            8 : '9. (exang) angina inducida por el ejercicio (1 = sí; 0 = no)',\n",
    "            9 : '10. (oldpeak) depresión del ST inducida por el ejercicio en relación con el reposo',\n",
    "            10 :'11. (pendiente) la pendiente del segmento ST de ejercicio máximo',\n",
    "            11 :'12. (ca) número de vasos principales (0-3) coloreados por fluoroscopia',\n",
    "            12 :'13. (thal) 3 = normal; 6 = defecto fijo; 7 = defecto reversible'}\n",
    "\n",
    "for x in range(0,len(sf.k_feature_idx_)):\n",
    "    print (dictChar[sf.k_feature_idx_[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
